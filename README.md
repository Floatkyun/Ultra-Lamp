# Ultra-Lamp（神灯）   
## 1 简介   
### 1.1 项目简介 
当今很多的年轻人一天大部分时间都在桌面前度过，而工作环境的优劣、工作学习习惯都会直接影响到人们的工作效率和健康状况，我们通过在边缘设备部署一套AI算法，实时地进行工作环境与工作者的工作状态的监测，并基于此为工作者提供个性化的工作环境评估与行动建议。而台灯作为一个常见的桌面物品，有着良好的位置优势，适合部署这样一套含有视觉输入的AI算法。另外提供守卫模式用于实时维护工作区域安全，接入大模型[ChatGLM4V](https://maas.aminer.cn/)为用户提供更智能的语音互动。    
### 1.2 功能与特性     
#### 1.2.1 灯光控制    
台灯亮度支持手动与自动调节两种方式，同时支持远程开关（这里的亮度值经过了伽马修正使其更线性符合人眼感受，同时我们的自动亮度曲线在黑暗环境下调节为较低的亮度，起到护眼的作用）。    
#### 1.2.2 环境状况监测与分析     
台灯搭载了一系列传感器采集使用者的工作环境状况，并将环境数据传入STM32F4中的LSTM，FCNN两个神经网络中，进行下一小时的温度预测与工作环境打分并显示在屏幕上。同时，光照，温度，二氧化碳浓度等信息也会实时上传物联网平台，可以在移动端APP和网页端查看。   
#### 1.2.3 桌面模式（重点功能）   
在屏幕上点击桌面模式按键，使用者可以根据实际，选择设置，在这种模式下，智能台灯通过摄像头捕获桌面状态，使用两个不同的[YOLOv5S](https://github.com/ultralytics/yolov5)模型，分别检测手的坐标信息与桌面物体，发送回STM32F4主控，通过PID算法实现平滑的手部跟踪，提高照明与工作状态检测效果。并通过判断桌面物体与手的交互，实时检测使用者的工作状态。我们的模型与算法能够检测六种状态：使用键盘，鼠标，看书，写字、玩手机以及喝水。足以涵盖绝大部分桌面工作场景。     
除了将工作状态直接统计显示外，为了提供更贴合感受的、人性化的状态显示、我们构建了工作状态检测模型，将检测到的各类工作状态综合计算并显示为“精力条”与“放松条”并显示在屏幕上，当精力过低或放松过高时，均会触发提醒。以及口渴值，当较长时间未喝水时，口渴值下降，下降过多后触发提醒。      
为了实现这一功能，我们构建了贴合桌面场景的手部识别与物体识别数据集，并且已经开源（见文档最后）。     
#### 1.2.4 守卫模式     
台灯开启守卫模式后，其会通过[YOLOv8-pose](https://github.com/ultralytics/ultralytics)模型开始守卫检测，当有人进入摄像头范围时，向使用者发送邮件提醒，使用者可以通过邮件查看照片，并且也可以点击链接，直接查看实时摄像监控，这样能够起到隐私守卫的作用。同时，若检测到的人处于摔倒状态，邮件将进行着重提醒。    
#### 1.2.5 智能对话     
在百度智能云[短语音识别](https://cloud.baidu.com/product/speech/asr)和[短文本在线合成](https://cloud.baidu.com/product/speech/tts_online)的支持下，台灯接入大模型[ChatGLM4V](https://maas.aminer.cn/)，用户可以通过语音向大模型提问。这一部分的功能参考了以下[教程代码](https://maixhub.com/share/37)，特别感谢。    

### 1.3主要性能指标    
两个设备上所部署的神经网络模型均经过量化（其中FCNN和LSTM模型经过[STM32Cube.AI](https://www.st.com.cn/content/st_com/zh/campaigns/stm32cube-ai.html)量化工具部署在STM32F4上；YOLOv5S和YOLOv8-pose模型经过[TPU-MLIR](https://github.com/sophgo/tpu-mlir)量化工具部署在SG2002上），在边缘设备上均表现出良好的性能，部署在STM32上的FCNN模型和LSTM模型的推理时间均在5ms左右，部署在SG2002上的YOLOv5S和YOLOv8-pose的推理时间均在20ms左右。桌面模式下同时运行两个YOLOv5S模型，推理速度可以长时间保持在25帧左右。

## 2 系统组成介绍
台灯主控采用STM32F407VET6，用于部署基于LSTM和FCNN的环境检测神经网络、驱动外设并用基于LVGL的图形界面与用户交互，另外使用[SG2002](https://wiki.sipeed.com/hardware/zh/maixcam/index.html)部署基于YOLOv5s和YOLOv8-pose的视觉神经网络、提供网络通信能力。整套系统借助物联网技术和流媒体传输技术实现远程控制和实时监控。    
其中STM32F407VET6使用裸板开发（考虑到神经网络对系统资源开销较大，未使用RTOS操作系统），SG2002使用[MaixPy](https://github.com/sipeed/MaixPy)固件提供的Linux环境开发，系统整体成本得到了很好的控制。    
系统框图如下图所示，下面对系统的各个部分进行介绍。    
![这是图片](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/sys_struct.png)     
### 2.1 整体系统介绍    
#### 2.1.1 硬件驱动介绍    
STM32F407VET6充当主控，其通过I2C总线与三个传感器相连来读取传感器；通过产生PWM波来控制LED调光和舵机云台转向，其中LED的PWM调光使用调光模块来完成，而舵机云台则与摄像头GC4653和LED灯相连，控制二者的指向；屏幕通过LGVL图形库进行渲染，由FSMC进行驱动,，其触摸IC则由SPI总线驱动（框图中未体现）。     
#### 2.1.2 环境信息读取与分析    
将采集到的传感器信息传给LSTM模型和FCNN进行分析，得到分析结果后将分析结果和采集到的数据打包发送给SG2002最后上传物联网平台，部分分析结果也会直接显示在TFT LCD屏幕上，供用户读取。    
#### 2.1.3 工作状态检测    
SG2002通过GC4653摄像头采集桌面图像，其中一个YOLOv5S模型负责通过采集到的图像准确的识别手部位置，并将位置信息经由卡尔曼滤波后通过串口传输到STM32F4VET6，STM32F4通过PID算法控制舵机云台使得台灯光源和摄像头可以一直追踪手部，为手部所处的工作区域提供持续的追踪光源，同时保证手部一直处于摄像头视野范围内，确保工作状态分析持续进行。    
而另外一个YOLOv5S模型则负责识别桌面物品和各自的位置，SG2002将这些识别结果整理后打包发送给STM32F4，STM32F4则结合这些信息由工作状态检测算法来统计分析得到当前使用者的工作状态，并经过LCD屏幕以条形图的方式显示出来。当某项工作状态超出阈值后将通过屏幕的方式提示使用者调整工作状态（框图中未体现）。    
#### 2.1.4 守卫模式     
SG2002此时将持续使用YOLOv8-pose模型检测人体和各个关键点的位置，并经过姿态检测算法得到人体目前的姿态信息（框图中未体现），如果识别到有人经过则会以邮件的方式通知使用者，并提供实时监控链接供使用者查看现场情况。    
![守卫模式邮件截图](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/mail.jpg)    
#### 2.1.5 云平台     
##### 2.1.5.1 [ThingsPanel物联网平台](https://www.thingspanel.cn/)    
由于服务器资源受限所以我们选择部署轻量化且功能较为丰富的[ThingsPanel开源物联网平台](https://www.thingspanel.cn/)，ThingsPanel云平台通过Docker安装的方式部署在我们的Linux服务器上，并通过MQTT协议与台灯进行通信，其支持通过移动端APP和网页端控制台灯功能（灯和监控的开关）、查看环境信息。    
![ThingsPanel平台截图](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/thingspanel.png)    
##### 2.1.5.2 [EasyDarwin流媒体平台](https://www.easydarwin.org/p/easydarwin.html)     
为了给台灯的监控提供推拉流服务，所以需要一个额外的流媒体平台，这里选择了[EasyDarwin流媒体平台](https://www.easydarwin.org/p/easydarwin.html)，其以直接运行的方式部署在我们的Linux服务器上，通过RTMP流媒体协议拉取台灯的视频流，并推流向使用者的移动设备，使得使用者在局域网以外也可以实时查看监控，该平台还提供录像自动保存功能，为使用者提供监控回放。    
![EasyDarwin平台截图](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/EasyDarwin.png)
#### 2.1.6 ChatGLM4V语言大模型    
用户点击“开始说话”按钮后，SG2002开始用板载麦克风录音（框图中未体现），说话完成后，点击“停止说话”后录音结束，随后将这段录音提交到百度智能云平台进行短语音识别，再将文字信息输提交到ChatGLM4V大模型中，等待模型生成文字后，再将文字提交到百度智能云平台进行短文本在线合成生成语音，最后经过音频功放由外接扬声器播放（框图中未体现）出来，实现一次对话。    
![开始说话、停止说话](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/Chat_begin_stop.png)     

### 2.2 硬件系统介绍    
#### 2.2.1 硬件整体介绍    
硬件部分，我们使用了STM32F407VET6作为主控，搭载核心算法，使用SG2002提供算力和网络支持。传感器包括AHT10温湿度传感器、SGP30二氧化碳传感器、GY-30光强传感器、GC4653摄像头。其他硬件模块包括悬挂舵机云台、PWM调光模块、TDA2030A功放模块、扬声器、电源模块。    
#### 2.2.2 机械设计介绍   
台灯灯柱与舵机云台直接使用购买的成品，而台灯灯头以及灯头与灯柱的连接结构均为自主设计。我们使用SketchUp进行机械结构的设计，并用3D打印方式进行制造。下面将分别对这两个部分进行介绍。
##### 2.2.2.1 台灯灯头    
台灯灯头的全局结构如图所示，其结构可以分为台灯头、灯罩、连接件三部分，台灯头正面与背面结构图如图和图所示。台灯灯头结构用来连接舵机与LED灯。设计能够稳定容纳LED灯与摄像头：大圆柱内为LED灯留出空间，小圆柱内为摄像头留出空间。整体结构为LED，摄像头，散热片，舵机，走线均提供了合理的空间。    
考虑到使用的LED功率较大，可能较为刺眼，我们为台灯头设计了可拆卸灯罩盖板，可将灯罩嵌入盖板中，自由选择灯罩的材质。而连接件则可以将灯罩与灯罩盖板固定起来。     
![灯头](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/3D_head.png)     
##### 2.2.2.2 灯头灯柱连接结构     
这部分的结构较为简单，只起到连接和固定作用。其结构图如图下所示。    
![灯头灯柱连接结构](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/3D_connector.png)    

#### 2.2.3 电路各模块介绍     
##### 2.2.3.1 传感器模块    
传感器包括AHT10温湿度传感器、SGP30二氧化碳传感器、GY-30光强传感器，其采集环境信息并通过I2C与单片机进行通信。    
##### 2.2.3.2 PWM调光模块    
通过输入不同占空比的方波来控制输出电压的大小，从而驱动大功率的LED灯。    
##### 2.2.3.3 TDA2030A功放模块    
功放模块负责放大声音信号，驱动扬声器。    
##### 2.2.3.4 电源模块    
电源模块可以接收12V供电作为输入，通过DC-DC芯片输出3.3V、5V和一个可调的高电压值。
### 2.3 软件系统介绍    
#### 2.3.1 软件整体介绍    
##### 2.3.1.1 STM32部分    
STM32上的软件程序主要包括以下几个模块：
传感器模块、舵机控制模块、LED控制模块、AI模块、工作状态检测与评估模块、屏幕图形操作界面模块。    
##### 2.3.1.2 SG2002部分
SG2002上的软件程序主要包括以下几个模块：
手部识别模块、桌面物品识别模块、人体检测与姿态识别模块、语言大模型模块、通信模块。以上模块之间没有联系，必须要与STM32配合才能工作。
#### 2.3.2 软件各模块介绍    
##### 2.3.2.1 STM32部分     
###### 2.3.2.1.1 传感器模块    
本模块的控制流程图如下图所示，包括SGP30气体传感器，AHT10温湿度传感器，GY30光传感器三部分驱动与功能函数，均通过IIC总线进行通信。函数通过定时器中断不断采集外界数据并存入变量中，用于后续的分析与处理。    
![传感器模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_sensor.png)     
###### 2.3.2.1.2 舵机控制模块    
本模块的控制流程图如下图所示，舵机控制函数模块接受摄像头采集到的“手”的坐标数据，通过PID算法，输出对PWM波的控制信号，控制舵机实现“灯”对“手”的平滑自动追踪。函数通过定时器中断循环运行，实现实时连续追踪。    
![舵机控制模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_servo.png)     
###### 2.3.2.1.3 LED控制模块     
本模块的控制流程图如下图所示：    
![舵机控制模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_led1.png)    
LED控制分为自动和手动两种模式，手动模式下，函数接受屏幕上亮度调节滑动条的值作为输入，输出对应占空比的PWM波，改变照明LED亮度；自动模式下，函数接受光传感器模块采集到的亮度信息作为输入，在额外通过一个调节函数（函数图像如下图所示，光线极弱时，台灯也会自动调小亮度，防止黑暗环境中光强过大伤害眼睛；当光线充足时，不再需要台灯的灯光，此时会将台灯亮度置零），输出对应PWM波，自动调节照明LED亮度。自动模式下，额外函数的功能使照明亮度与外界亮度间不只为简单的反比互补关系，而在外界较为黑暗时，能够稍调小亮度以护眼。    
![舵机控制模块_](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_led2.png)    
伽马修正函数使得亮度设置与人眼感知到的光强之间为线性关系，照明调节更符合体感。函数通过定时器中断循环运行，实现实时连续照明调节。    
###### 2.3.2.1.4 AI模块    
本模块的控制流程图如下图所示：    
![AI模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_ai1.png)     
单片机上部署两个轻量化神经网络，接受传感器检测到的工作环境信息，输出温度预测与环境评分，通过LVGL图形库显示在屏幕上。函数通过定时器中断循环运行，保证实时性。两个AI模型的结构分别如下图所示。    
![AI模块_](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_ai2.png)     
###### 2.3.2.1.5 工作状态检测与评估模块    
模块整体流程图如下图所示，这个模块为“工作模式”下核心模块。启动“工作模式”后，每分钟（演示模式下为5秒）接收来自SG2002的工作状态判断结果，经过一平滑滤波防止意外的状态中断，再作为函数输入。函数内部进行分析后，输出：连续工作时间、连续游戏时间、喝水次数、精力值、放松值、口渴值六路结果。    
![工作状态检测与评估模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_work_detector1.png)    
放松值计算算法流程如下图所示，其中，精力值函数接受连续工作时长、连续游戏时长，当前工作状态为输入，输出一个直观的精力值，具体表现为当连续工作时间越长，精力值下降越快；连续游戏时长超过一定值，玩游戏由增加精力值逐渐变为消耗精力值。此外，休息（与指定物品集无交互）状态使精力值线性缓慢升高。     
![工作状态检测与评估模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_work_detector2.png)      
放松值计算算法流程如下图所示，放松值函数接受连续游戏时长，当前工作状态为输入，输出一个直观的放松值，具体表现为当连续游戏时长越长，放松值增长越快，此外，休息（与指定物品集无交互）状态使放松值线性缓慢升高，工作状态使放松值线性较快降低。    
![工作状态检测与评估模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_work_detector3.png)    
口渴值计算算法流程如下图所示，口渴值随时间线性缓慢降低，口渴值函数接受当前工作状态为输入，检测到喝水状态使口渴值升高。整体模块在接收到SG2002包含工作状态序列的串口信息时触发。    
![工作状态检测与评估模块](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/soft_work_detector4.png)    
###### 2.3.2.1.6 工作状态检测与评估模块     
屏幕的图形界面使用GUI_Guider进行设计并用LVGL图形库进行渲染，在按键回调函数中调用对应的功能函数，实现近场的全面触屏控制。   
##### 2.3.2.1 SG20202部分    
由于主要的算法逻辑都集中在STM32F4开发板上，这一部分的算法逻辑较简单，有一部分可以直接使用MaixPy固件及其运行库实现，不在这里展示。    
###### 2.3.2.2.1 手部识别模块     
手部识别算法的核心是部署在SG2002上的YOLOv5S手部识别模型，并将识别结果坐标与图片中心点的偏移量经过卡尔曼滤波后通过串口发送给STM32F4，其再用舵机控制模块来使摄像头可以追踪手部。    
为了得到一个在桌面场景下效果良好的手部识别模型，我们整理和过滤（边界框过大或过小的样本被剔除）了5个搜索到的手部数据集，得到了适用于桌面场景的手部识别数据集，该数据集的标签均已转化为YOLO格式，目前已经开放供大家开发智能桌面应用。    
###### 2.3.2.2.2 桌面物品识别模块     
桌面物品识别算法的核心是部署在SG2002上的YOLOv5S桌面物品识别模型，并利用手部识别模块得到的手部边界框，计算其与桌面物品的边界框的交叠面积占手部边界框面积的比例，根据这一比例的大小来得到手部是否与该物品接触的判断，将这一系列的判断结果打包后发送给STM32F4进行进一步的分析。     
为了得到一个在桌面场景下效果良好的桌面物品识别模型，我们整理和过滤（边界框过大或过小的样本被剔除）了6个物品识别数据集（包括COCO 2017和PASCAL VOC 2012数据集），得到了适用于桌面场景的桌面物品识别数据集，数据集中的桌面物品包括电脑键盘、鼠标、水杯、手机、书、笔共六个物品，该数据集的标签均已转化为YOLO格式，目前已经开放供大家开发智能桌面应用。     
###### 2.3.2.2.3 人体检测与姿态识别模块    
人体检测与姿态识别算法的核心是部署在SG2002上的YOLOv8-pose人体关键点模型，利用这一模型得到人体的关键点信息，再经过姿态识别算法处理这些关键点信息得到人体目前的姿态（站立、坐、跌倒、其他共四种状态）。	在守卫模式下，如果检测到人体信息，则会利用email python库发送台灯所拍摄的照片和一个监控推流网址（如果有人跌倒也会发送人的姿态状况），点击网址后可以看到实时的监控情况（由于使用rtmp协议，所以监控的延迟比较难以控制），邮件的格式如下图所示。      
![守卫模式邮件截图](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/mail.jpg)      
###### 2.3.2.2.4 语言大模型模块      
在这一部分，首先使用SG2002板载麦克风进行录音，随后将音频上传到百度智能云进行语音转文字操作，随后将得到的文字内容上传到智谱大模型平台接收语言大模型的回复，再将收到的回复文字通过百度智能云平台生成语音，最后使用扬声器播放出来。    
###### 2.3.2.2.5 通信模块    
与物联网通信使用paho-mqtt python库进行MQTT协议通信，通过MaixPy库实现用rtmp协议推流到流媒体平台，其他网络通信均使用requests python库实现。    
## 3 实物成果    
### 3.1 整体介绍    
系统正面、侧面的照片如下图所示。两个开发板、电源模块、扬声器、屏幕等直接置于桌面上，PWM调光模块置于灯柱上，LED、舵机云台和摄像头置于灯头上。   
![系统正面、侧面的照片](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/real_whole.png)      
### 3.2 工程成果    
#### 3.2.1 机械成果    
灯头的正面侧面背面如下图所示。    
![灯头的正面侧面背面](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/real_head.png)     
灯头灯柱连接部分如下图所示。   
![灯头的正面侧面背面](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/real_connector.png)     

#### 3.2.2 电路成果    
I2C转接板实际图片和排针转段子转接板实际图片如下图所示，其他电路模块均为购买。    
![灯头的正面侧面背面](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/real_circuit.png)     
#### 3.2.3 软件成果    
台灯操作界面如下图所示，图中所有实际数据用“-”代替。    
![灯头的正面侧面背面](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/real_soft.png)   

### 3.3特性成果    
主要功能无法使用图片来展示，所以建议直接参考演示视频（近期可能会上传）。


## 附件：开源数据集   
桌面场景人工智能应用的开发离不开对应的高质量数据集，我们从各大数据集中采集、过滤得到了一批适用于桌面场景的数据集。   
数据集标注方式采用YOLO格式，可以方便的用于边缘人工智能的训练。   
- [桌面场景物品识别（detect）数据集](https://www.123pan.com/s/SJYZVv-7ZrQ.html)
  包含以下几个常见物品：
  - 书
  - 手机
  - 键盘
  - 鼠标
  - 笔
  - 水杯
![这是图片](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/labels_obj.jpg)
  
- [桌面场景手部识别（detect）数据集](https://www.123pan.com/s/SJYZVv-ZZrQ.html)
  包含以下两个常见手势：
  - 无手势
  - OK手势
![这是图片](https://github.com/Floatkyun/Ultra-Lamp/blob/main/image/labels_hand.jpg)


